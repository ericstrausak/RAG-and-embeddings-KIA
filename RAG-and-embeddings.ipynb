{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a FAISS-Based Vector Store: A Journey Through Data Processing and Visualization\n",
    "\n",
    "In this notebook, you'll learn how to transform raw PDF documents into a searchable vector store using FAISS. We'll go on a journey where we:\n",
    "\n",
    "1. **Read and extract text from PDF files.**\n",
    "2. **Split the text into manageable chunks.**\n",
    "3. **Display tokenization outputs from different tokenizers.**\n",
    "4. **Generate embeddings from the text using a SentenceTransformer.**\n",
    "5. **Store the embeddings in a FAISS index.**\n",
    "6. **Project the embeddings into 2D space using UMAP for visualization.**\n",
    "7. **Visualize the entire process on a scatter plot.**\n",
    "8. **Incect your data into a prompt for a large language model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # For generating embeddings for text chunks\n",
    "import faiss\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Data from PDFs\n",
    "\n",
    "First, we load PDF files from a directory, extract their text content, and combine it into one large text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the pdf from the path\n",
    "glob_path = \"data/*.pdf\"\n",
    "text = \"\"\n",
    "for pdf_path in tqdm.tqdm(glob.glob(glob_path)):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        print(file)\n",
    "        reader = PdfReader(file)\n",
    "         # Extract text from all pages in the PDF\n",
    "        text += \" \".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "\n",
    "text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Splitting the Text into Chunks\n",
    "\n",
    "Large texts can be difficult to work with. We use a text splitter, in this case [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/how_to/recursive_text_splitter/),  to break the full text into smaller, overlapping chunks. This helps preserve context when we later embed the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a splitter: 2000 characters per chunk with an overlap of 200 characters\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "# Split the extracted text into manageable chunks\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(\"Preview of the first chunk:\", chunks[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizing the Text with Different Tokenizers\n",
    "\n",
    "Before embedding, it's insightful to see how different tokenizers break up our text. Here, we use the tokenizer from the SentenceTransformer model (see [SentenceTransformersTokenTextSplitter](https://python.langchain.com/api_reference/text_splitters/sentence_transformers/langchain_text_splitters.sentence_transformers.SentenceTransformersTokenTextSplitter.html#sentencetransformerstokentextsplitter))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=128, model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_split_texts = []\n",
    "for text in chunks:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")\n",
    "print(token_split_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenized_chunks = []\n",
    "for i, text in enumerate(token_split_texts[:10]):\n",
    "    # Tokenize each chunk\n",
    "    encoded_input = model.tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0].tolist())\n",
    "    tokenized_chunks.append(tokens)\n",
    "    print(f\"Chunk {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Sahajtomar/German-semantic\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenized_chunks = []\n",
    "for i, text in enumerate(token_split_texts[:10]):\n",
    "    # Tokenize each chunk\n",
    "    encoded_input = model.tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0].tolist())\n",
    "    tokenized_chunks.append(tokens)\n",
    "    print(f\"Chunk {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating Embeddings for Each Chunk\n",
    "\n",
    "Now we convert each text chunk into a numerical embedding that captures its semantic meaning. These embeddings will be used for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "chunk_embeddings = model.encode(token_split_texts, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building a FAISS Vector Store\n",
    "\n",
    "FAISS is a powerful library for efficient similarity search. Here, we build an index from our embeddings. Remember, FAISS only stores the numerical vectors so we must keep our original text mapping separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = chunk_embeddings.shape[1]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(chunk_embeddings)\n",
    "print(\"Number of embeddings in FAISS index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('faiss'):\n",
    "    os.makedirs('faiss')\n",
    "    \n",
    "faiss.write_index(index, \"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_2 = faiss.read_index(\"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"rb\") as f:\n",
    "    token_split_texts_2 = pickle.load(f)\n",
    "print(len(token_split_texts_2))\n",
    "print(len(token_split_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Projecting Embeddings with UMAP\n",
    "\n",
    "To visualize high-dimensional embeddings, we use UMAP to project them into 2D space. You can project both the entire dataset and individual query embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit UMAP on the full dataset embeddings\n",
    "umap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(chunk_embeddings)\n",
    "\n",
    "def project_embeddings(embeddings, umap_transform):\n",
    "    \"\"\"\n",
    "    Project a set of embeddings using a pre-fitted UMAP transform.\n",
    "    \"\"\"\n",
    "    umap_embeddings = np.empty((len(embeddings), 2))\n",
    "    for i, embedding in enumerate(tqdm.tqdm(embeddings, desc=\"Projecting Embeddings\")):\n",
    "        umap_embeddings[i] = umap_transform.transform([embedding])\n",
    "    return umap_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the entire dataset embeddings\n",
    "projected_dataset_embeddings = project_embeddings(chunk_embeddings, umap_transform)\n",
    "print(\"Projected dataset embeddings shape:\", projected_dataset_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Querying the Vector Store and Projecting Results\n",
    "\n",
    "We now define a retrieval function that takes a text query, embeds it, and searches our FAISS index for similar documents. We then project these result embeddings with UMAP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve the top k similar text chunks and their embeddings for a given query.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    retrieved_texts = [token_split_texts[i] for i in indices[0]]\n",
    "    retrieved_embeddings = np.array([chunk_embeddings[i] for i in indices[0]])\n",
    "    return retrieved_texts, retrieved_embeddings, distances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"KI während der Bachelorarbeit\"\n",
    "results, result_embeddings, distances = retrieve(query, k=3)\n",
    "print(\"Retrieved document preview:\")\n",
    "print(results[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the result embeddings\n",
    "projected_result_embeddings = project_embeddings(result_embeddings, umap_transform)\n",
    "\n",
    "# Also embed and project the original query for visualization\n",
    "query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "project_original_query = project_embeddings(query_embedding, umap_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing the Results\n",
    "\n",
    "Finally, we create a scatter plot to visualize the entire dataset, the retrieved results, and the original query in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shorten_text(text, max_length=15):\n",
    "    \"\"\"Shortens text to max_length and adds an ellipsis if shortened.\"\"\"\n",
    "    return (text[:max_length] + '...') if len(text) > max_length else text\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Scatter plots\n",
    "plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1],\n",
    "            s=10, color='gray', label='Dataset')\n",
    "plt.scatter(projected_result_embeddings[:, 0], projected_result_embeddings[:, 1],\n",
    "            s=100, facecolors='none', edgecolors='g', label='Results')\n",
    "plt.scatter(project_original_query[:, 0], project_original_query[:, 1],\n",
    "            s=150, marker='X', color='r', label='Original Query')\n",
    "\n",
    "# If results is a list of texts, iterate directly\n",
    "for i, text in enumerate(results):\n",
    "    if i < len(projected_result_embeddings):\n",
    "        plt.annotate(shorten_text(text),\n",
    "                     (projected_result_embeddings[i, 0], projected_result_embeddings[i, 1]),\n",
    "                     fontsize=8)\n",
    "\n",
    "# Annotate the original query point\n",
    "original_query_text = 'Welche hilfsmittel sind erlaubt?'  # Replace with your actual query text if needed\n",
    "original_query_text = 'Wieviele Seiten muss die Arbeit sein?'  # Replace with your actual query text if needed\n",
    "\n",
    "plt.annotate(shorten_text(original_query_text),\n",
    "             (project_original_query[0, 0], project_original_query[0, 1]),\n",
    "             fontsize=8)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('Visualization')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📝 Task: Semantic Retrieval-Augmented Question Answering Using Groq LLM\n",
    "\n",
    "## Objective\n",
    "Implement a question-answering system that:\n",
    "1. Retrieves the most semantically relevant text passages to a user query.\n",
    "2. Constructs a natural language prompt based on the retrieved content.\n",
    "3. Uses a large language model (LLM) hosted by Groq to generate an answer.\n",
    "\n",
    "---\n",
    "\n",
    "## Task Breakdown\n",
    "\n",
    "### 1. Embedding-Based Semantic Retrieval\n",
    "- Use the `SentenceTransformer` model `\"Sahajtomar/German-semantic\"` to encode a user query into a dense vector embedding.\n",
    "- Perform a nearest-neighbor search in a prebuilt FAISS index to retrieve the top-**k** similar text chunks. You can **use the prebuilt FAISS form above**.\n",
    "\n",
    "\n",
    "### 2. LLM Prompt Construction and Query Answering\n",
    "- Build the prompt:\n",
    "  - Using the retrieved text chunks, concatenates the results into a context block.\n",
    "  - Builds a **prompt** asking the LLM to answer the question using that context.\n",
    "  - Sends the prompt to the **Groq LLM API** (`llama-3.3-70b-versatile`) and returns the response.\n",
    "\n",
    "### 3. User Query Execution\n",
    "- An example query (`\"What is the most important factor in diagnosing asthma?\"`) is used to demonstrate the pipeline.\n",
    "- The final answer from the LLM is printed.\n",
    "\n",
    "\n",
    "## Tools & Models Used\n",
    "- **SentenceTransformers** (`Sahajtomar/German-semantic`) for embedding generation.\n",
    "- **FAISS** for efficient vector similarity search.\n",
    "- **Groq LLM API** (`llama-3.3-70b-versatile`) for generating the final response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# Access the API key using the variable name defined in the .env file\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.1 (main, Mar 17 2025, 17:13:06) [GCC 9.4.0]\n",
      "Python executable: /home/codespace/.python/current/bin/python3\n",
      "Initializing SimpleRAG system...\n",
      "Loading model: Sahajtomar/German-semantic\n",
      "Groq client initialized\n",
      "Using sample text chunks\n",
      "Loading 5 chunks...\n",
      "Processing batch 1/1...\n",
      "Generated embeddings with shape (5, 1024)\n",
      "\n",
      "==================================================\n",
      "Running example queries:\n",
      "\n",
      "Query: What is the most important factor in diagnosing asthma?\n",
      "Processing query: What is the most important factor in diagnosing asthma?\n",
      "Retrieved 3 chunks:\n",
      "Chunk 1 (similarity: 0.1472): KI kann während der Bachelorarbeit als nützliches ...\n",
      "Chunk 2 (similarity: 0.1456): Beim Schreiben der Bachelorarbeit ist auf wissensc...\n",
      "Chunk 3 (similarity: 0.1408): Die Gliederung einer Bachelorarbeit umfasst typisc...\n",
      "Generating answer...\n",
      "\n",
      "Answer:\n",
      "--------------------------------------------------\n",
      "Es gibt keine Informationen im Kontext, die auf die Diagnose von Asthma eingehen. Der Kontext behandelt stattdessen die Erstellung einer Bachelorarbeit, insbesondere die Verwendung von KI bei der Literaturrecherche, die Bedeutung wissenschaftlicher Sprache und korrekter Zitation sowie die typische Gliederung einer Bachelorarbeit. Daher kann keine Antwort auf die Frage zur Diagnose von Asthma gegeben werden.\n",
      "--------------------------------------------------\n",
      "\n",
      "Query: KI während der Bachelorarbeit\n",
      "Processing query: KI während der Bachelorarbeit\n",
      "Retrieved 3 chunks:\n",
      "Chunk 1 (similarity: 0.7451): KI kann während der Bachelorarbeit als nützliches ...\n",
      "Chunk 2 (similarity: 0.3355): Eine Bachelorarbeit umfasst typischerweise zwische...\n",
      "Chunk 3 (similarity: 0.3326): Die Formatierung der Bachelorarbeit folgt den Vorg...\n",
      "Generating answer...\n",
      "\n",
      "Answer:\n",
      "--------------------------------------------------\n",
      "KI kann während der Bachelorarbeit als nützliches Tool eingesetzt werden, um Literaturrecherche zu unterstützen.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# This is a minimal RAG implementation using in-memory vector storage instead of FAISS\n",
    "# It's designed to work with minimal dependencies and be less prone to kernel crashes\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Print Python version and environment info for debugging\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "# Define a minimal test dataset if we can't use existing data\n",
    "sample_chunks = [\n",
    "    \"KI kann während der Bachelorarbeit als nützliches Tool eingesetzt werden, um Literaturrecherche zu unterstützen.\",\n",
    "    \"Die Formatierung der Bachelorarbeit folgt den Vorgaben der Universität und sollte konsistent sein.\",\n",
    "    \"Eine Bachelorarbeit umfasst typischerweise zwischen 40 und 60 Seiten, abhängig von den spezifischen Anforderungen.\",\n",
    "    \"Beim Schreiben der Bachelorarbeit ist auf wissenschaftliche Sprache und korrekte Zitation zu achten.\",\n",
    "    \"Die Gliederung einer Bachelorarbeit umfasst typischerweise: Einleitung, Hauptteil, Fazit und Literaturverzeichnis.\"\n",
    "]\n",
    "\n",
    "class SimpleRAG:\n",
    "    \"\"\"\n",
    "    A simplified RAG implementation that doesn't rely on FAISS or other complex libraries.\n",
    "    Uses in-memory vector storage and cosine similarity for retrieval.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"Sahajtomar/German-semantic\"):\n",
    "        \"\"\"Initialize the RAG system with a SentenceTransformer model.\"\"\"\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.chunks = []\n",
    "        self.embeddings = None\n",
    "        self.groq_client = None\n",
    "        if groq_api_key:\n",
    "            self.groq_client = Groq(api_key=groq_api_key)\n",
    "            print(\"Groq client initialized\")\n",
    "        else:\n",
    "            print(\"Warning: No GROQ_API_KEY found. LLM functionality will be unavailable.\")\n",
    "    \n",
    "    def load_chunks(self, text_chunks):\n",
    "        \"\"\"Load text chunks and compute embeddings.\"\"\"\n",
    "        print(f\"Loading {len(text_chunks)} chunks...\")\n",
    "        self.chunks = text_chunks\n",
    "        # Process chunks in small batches to avoid memory issues\n",
    "        batch_size = 32\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(self.chunks), batch_size):\n",
    "            batch = self.chunks[i:i+batch_size]\n",
    "            print(f\"Processing batch {i//batch_size + 1}/{(len(self.chunks)-1)//batch_size + 1}...\")\n",
    "            batch_embeddings = self.model.encode(batch, convert_to_numpy=True)\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        self.embeddings = np.vstack(all_embeddings)\n",
    "        print(f\"Generated embeddings with shape {self.embeddings.shape}\")\n",
    "    \n",
    "    def cosine_similarity(self, a, b):\n",
    "        \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    \n",
    "    def retrieve(self, query, k=3):\n",
    "        \"\"\"Retrieve the most similar chunks to the query.\"\"\"\n",
    "        # Encode the query\n",
    "        query_embedding = self.model.encode([query], convert_to_numpy=True)[0]\n",
    "        \n",
    "        # Compute similarities\n",
    "        similarities = []\n",
    "        for i, embedding in enumerate(self.embeddings):\n",
    "            similarity = self.cosine_similarity(query_embedding, embedding)\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top-k chunks\n",
    "        results = []\n",
    "        for i, similarity in similarities[:k]:\n",
    "            results.append((self.chunks[i], similarity))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_answer(self, query, retrieved_chunks):\n",
    "        \"\"\"Generate an answer using the Groq LLM API.\"\"\"\n",
    "        if not self.groq_client:\n",
    "            return \"LLM functionality unavailable. Please set GROQ_API_KEY.\"\n",
    "        \n",
    "        # Prepare context from retrieved chunks\n",
    "        context = \"\\n\\n---\\n\\n\".join([chunk for chunk, _ in retrieved_chunks])\n",
    "        \n",
    "        # Build the prompt\n",
    "        prompt = f\"\"\"Answer the following question based on the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "        \n",
    "        # Query the LLM\n",
    "        try:\n",
    "            completion = self.groq_client.chat.completions.create(\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            return completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling Groq API: {e}\")\n",
    "            return f\"Error generating answer: {str(e)}\"\n",
    "    \n",
    "    def answer_question(self, query, k=3):\n",
    "        \"\"\"End-to-end question answering.\"\"\"\n",
    "        print(f\"Processing query: {query}\")\n",
    "        \n",
    "        # Retrieve similar chunks\n",
    "        retrieved_chunks = self.retrieve(query, k)\n",
    "        \n",
    "        print(f\"Retrieved {len(retrieved_chunks)} chunks:\")\n",
    "        for i, (chunk, similarity) in enumerate(retrieved_chunks):\n",
    "            preview = chunk[:50].replace('\\n', ' ') + '...'\n",
    "            print(f\"Chunk {i+1} (similarity: {similarity:.4f}): {preview}\")\n",
    "        \n",
    "        # Generate answer\n",
    "        print(\"Generating answer...\")\n",
    "        answer = self.generate_answer(query, retrieved_chunks)\n",
    "        \n",
    "        return answer\n",
    "\n",
    "# Main execution\n",
    "try:\n",
    "    print(\"Initializing SimpleRAG system...\")\n",
    "    rag = SimpleRAG()\n",
    "    \n",
    "    # Try to use chunks from the notebook, fall back to sample data if not available\n",
    "    text_chunks = None\n",
    "    try:\n",
    "        # Try to access variables from the notebook\n",
    "        if 'token_split_texts' in globals():\n",
    "            text_chunks = token_split_texts\n",
    "            print(f\"Using token_split_texts with {len(text_chunks)} chunks\")\n",
    "        elif 'chunks' in globals():\n",
    "            text_chunks = chunks\n",
    "            print(f\"Using chunks with {len(text_chunks)} chunks\")\n",
    "    except NameError:\n",
    "        pass\n",
    "    \n",
    "    # If no chunks found, use the sample data\n",
    "    if not text_chunks:\n",
    "        print(\"Using sample text chunks\")\n",
    "        text_chunks = sample_chunks\n",
    "    \n",
    "    # Limit the number of chunks for stability\n",
    "    max_chunks = 1000\n",
    "    if len(text_chunks) > max_chunks:\n",
    "        print(f\"Too many chunks ({len(text_chunks)}). Using first {max_chunks} chunks.\")\n",
    "        text_chunks = text_chunks[:max_chunks]\n",
    "    \n",
    "    # Load the chunks and create embeddings\n",
    "    rag.load_chunks(text_chunks)\n",
    "    \n",
    "    # Example queries\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Running example queries:\")\n",
    "    \n",
    "    # Example query from the task\n",
    "    example_query = \"What is the most important factor in diagnosing asthma?\"\n",
    "    print(f\"\\nQuery: {example_query}\")\n",
    "    answer = rag.answer_question(example_query)\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(answer)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # German query\n",
    "    german_query = \"KI während der Bachelorarbeit\"\n",
    "    print(f\"\\nQuery: {german_query}\")\n",
    "    german_answer = rag.answer_question(german_query)\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(german_answer)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
